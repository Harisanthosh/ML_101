{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Recommendation system based on PySpark"},{"metadata":{},"cell_type":"markdown","source":"In this tutorial I am going to demonstrate how Spark MLlib can be used for movie recommendation task.\n\nFirst, let me start from problem statement: goal of any recommendation system is to analyze how users interact with existing system and make some recommendation of what they can buy, listen or watch in the future so that we can make our users happier, and hopefully, increase profitability of system we develop.\n\nThere are several approaches to this task due to [wiki](https://en.wikipedia.org/wiki/Recommender_system): \n* Collaborative filtering \nmethods are based on collecting and analyzing a large amount of information on users’ behaviors, activities or preferences and predicting what users will like based on their similarity to other users. A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself. Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past.\n* Content-based filtering\nmethods are based on a description of the item and a profile of the user’s preferences.\nIn a content-based recommender system, keywords are used to describe the items and a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended.\n* Hybrid recommender systems\nwhich is combining collaborative filtering and content-based filtering could be more effective in some cases. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one mode. "},{"metadata":{},"cell_type":"markdown","source":"In this work I will demonstrate how to use Collaborative Filtering approach using Spark MLlib. "},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{},"cell_type":"markdown","source":"In my case I would like to recommend movies to users, based on dataset provided by [Megogo](https://megogo.net/ru) and competition they hosted at [Kaggle](https://www.kaggle.com/c/megogochallenge). \n\nI've been participated in this competiton during mlcourse and took 10th place out of 42 teams there. "},{"metadata":{},"cell_type":"markdown","source":"Displaying data so that you could undesrtand how it looks like: "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\n\n%env JOBLIB_TEMP_FOLDER=/tmp \n#https://www.kaggle.com/getting-started/45288 - this helps some with 'no space left on device'\n\nprint(os.listdir(\"../input\"))","execution_count":3,"outputs":[{"output_type":"stream","text":"env: JOBLIB_TEMP_FOLDER=/tmp\n[]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd ../input && ls -la","execution_count":11,"outputs":[{"output_type":"stream","text":"total 24\r\ndrwxr-xr-x 6 root root 4096 Nov 25 05:15 .\r\ndrwxr-xr-x 1 root root 4096 Nov 25 05:15 ..\r\ndrwxr-xr-x 2 root root 4096 Nov 25 05:15 config\r\ndrwxr-xr-x 2 root root 4096 Nov  7 15:34 input\r\ndrwxr-xr-x 3 root root 4096 Oct 16 01:33 lib\r\ndrwxr-xr-x 3 root root 4096 Nov 25 05:15 working\r\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"full_df = pd.read_csv('../input/megogochallenge/train_data_full.csv')\nfull_df.head()","execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"File b'../input/megogochallenge/train_data_full.csv' does not exist","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-15c99a03971c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/megogochallenge/train_data_full.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/megogochallenge/train_data_full.csv' does not exist"]}]},{"metadata":{},"cell_type":"markdown","source":"It's very important not only to recommend movies that user would like to watch. But to recommend movies which user will watch more than on half. It could be observed in `watching_percentage` column. \n\nActually it was a goal of competition (to predict movies that will be watched more than on half). "},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df['watching_percentage'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most movies are watched till the end."},{"metadata":{},"cell_type":"markdown","source":"## Metric"},{"metadata":{},"cell_type":"markdown","source":"As evaluation metric MAP@10 is used for competiton - it's variations is very common metric for any recommendation tasks. More about it you could read [here](http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html).\n\nBut in this tutorial I'll use RMSE, which is easier to understand for everyone."},{"metadata":{},"cell_type":"markdown","source":"# PySpark implementation"},{"metadata":{},"cell_type":"markdown","source":"Finally, let's move to PySpark. \nThere is 2 ways to install it in Kaggle Kernels: \n* Run magic command, as in cell below\n* Or go to Packages and enter pyspark (it will take some time, but this will form your own version of docker and you'll no need to waste time while running next versions of kernel)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To install it locally or at jupyter server follow [this](https://medium.freecodecamp.org/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389) instruction. \n\nFirst, you have to install Java and Scala. Then set enviroment variables to launch Spark with Python3 and finally install pyspark."},{"metadata":{},"cell_type":"markdown","source":"Now importing all needed spark modules. Pay attention how sc and spark variables were initialized. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nsc = SparkContext('local') #https://stackoverflow.com/questions/30763951/spark-context-sc-not-defined\nspark = SparkSession(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's read data in Spark format: "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_schema = StructType([\n    StructField('session_start_datetime',TimestampType(), False),\n    StructField('user_id',IntegerType(), False),\n    StructField('user_ip',IntegerType(), False),\n    StructField('primary_video_id',IntegerType(), False),\n    StructField('video_id',IntegerType(), False),\n    StructField('vod_type',StringType(), False),\n    StructField('session_duration',IntegerType(), False),\n    StructField('device_type',StringType(), False),\n    StructField('device_os',StringType(), False),\n    StructField('player_position_min',LongType(), False),\n    StructField('player_position_max',LongType(), False),\n    StructField('time_cumsum_max',LongType(), False),\n    StructField('video_duration',IntegerType(), False),\n    StructField('watching_percentage',FloatType(), False)\n])\nfinal_stat = spark.read.csv(\n    '../input/megogochallenge/train_data_full.csv', header=True, schema=data_schema\n).cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's prepare data for model: "},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings = (final_stat\n    .select(\n        'user_id',\n        'primary_video_id',\n        'watching_percentage',\n    )\n).cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a `train_test_split`: "},{"metadata":{"trusted":true},"cell_type":"code","source":"(training, test) = ratings.randomSplit([0.8, 0.2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)Finally build an [ALS](https://dl.acm.org/citation.cfm?id=1608614) model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=2, regParam=0.01, \n          userCol=\"user_id\", itemCol=\"primary_video_id\", ratingCol=\"watching_percentage\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=True)\nmodel = als.fit(training)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watching_percentage\",\n                                predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on parameters, and try to find any improvements. \n\nParameters of ALS Model in PySpark realization are following: \n* **NumBlocks** is the number of blocks the users and items will be partitioned into in order to parallelize computation.\n* **rank** is the number of latent factors in the model.\n* **maxIter** is the maximum number of iterations to run.\n* **regParam** specifies the regularization parameter in ALS.\n* **implicitPrefs** specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n* **alpha** is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0)."},{"metadata":{},"cell_type":"markdown","source":"## Explicit or implicit? \n\nThe standard approach to matrix factorization based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item, for example, users giving ratings to movies.\n\nAs we see in our dataset we have a bunch of implicit information like `device_type`, `video_duration`, `device_os`. \nBut let's try to use only explicit information and look on RMSE."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=2, regParam=0.01, \n          userCol=\"user_id\", itemCol=\"primary_video_id\", ratingCol=\"watching_percentage\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=False) #changed param!\nmodel = als.fit(training)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watching_percentage\",\n                                predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! Our RMSE improved really well!\nBut remember, if the rating matrix is derived from another source of information (i.e. it is inferred from other signals), you can set `implicitPrefs` to `True` to get better results. \n\nCould we do more? \nLet me increase `rank`, which is number of latent factor a bit. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(rank=20, #10 was by default\n          maxIter=2, regParam=0.01,\n          userCol=\"user_id\", itemCol=\"primary_video_id\", ratingCol=\"watching_percentage\",\n          coldStartStrategy=\"drop\",\n          implicitPrefs=False)\nmodel = als.fit(training)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watching_percentage\",\n                                predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Our score is improved a bit, but not dramatically. \n\nHere I should say that I am a bit limited by Kaggle Kernels resources (Spark use a lot of RAM!) and of course it's better to experiment with parameters locally.\n\nGeneral recommendations: \n* increase `maxIter` and `rank` checking them on CV of course (may be time and RAM consuming). \n* don't forget about regularization parametr\n\nOkay, now I want to output the recommendations itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Generate top 10 movie recommendations for each user\nuserRecs = model.recommendForAllUsers(10)\nuserRecs.count()\n# Generate top 10 user recommendations for each movie\nmovieRecs = model.recommendForAllItems(10)\nmovieRecs.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting Spark data to well-known Pandas could be done easily with `toPandas()` method: "},{"metadata":{"trusted":true},"cell_type":"code","source":"userRecs_df = userRecs.toPandas()\nprint(userRecs_df.shape)\n\nmovieRecs_df = movieRecs.toPandas()\nprint(movieRecs_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"userRecs_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see movie recommendation and it's score by each user."},{"metadata":{"trusted":true},"cell_type":"code","source":"movieRecs_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see recommendation and it's score by each movie."},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"In summary, collaborative filtering is one of the most popular approach to build recommendation system. \nMovie recommendation task become extremely popular after [Netflix competition](https://en.wikipedia.org/wiki/Netflix_Prize) with one million dollar prize, which push Machine Learning a lot in recommender systems field. \n\nWith PySpark, we could get great results in this task just in a few lines of code. \nIn production, new problems appeared, on of them is **cold start** problem when we have no any historical information about user, but still have to recommend something.\nBut that's more than this tutorial would like to cover.\n\nHope this small intro was useful, feel free to criticize, discuss, and maybe upvote:) "},{"metadata":{},"cell_type":"markdown","source":"## References"},{"metadata":{},"cell_type":"markdown","source":"* [Spark docs](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html)\n* [Megogo challenge baselines](https://github.com/SantyagoSeaman/megogo_challenge_solutions)\n* [More on ranking metrics](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics)\n* [How does netflix recommeder system works](https://help.netflix.com/en/node/100639)\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}